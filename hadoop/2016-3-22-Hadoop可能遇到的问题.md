#Hadoop可能遇到的问题
Q1:启动hadoop时提示running as process Stop it first

A:关闭所有hadoop进程再启动
`stop-all.sh`
`start-all.sh`
    
Q2:启动hadoop后namenode没有启动,namenode日志报错:checkpoint directory does not exist or is not accessible.

A:如果dfs/name文件夹已存在,那可能是没有权限读取该文件夹,通过赋予hadoop用户改文件夹权限来解决:
`su root`
`chown -R hadoop /opt/hadoop-2.6.0-cdh5.6.0/dfs/name`

Q3:datanode没有启动,hadoop-hadoop-datanode-slave1.log日志报错
Incompatible clusterIDs in /usr/local/hadoop/tmp/dfs/data: namenode clusterID = CID-8e201022-6faa-440a-b61c-290e4ccfb00...

A:原因是`hdfs namenode -format`只会格式化namenode,并不会影响到datanode,如果再次格式化会导致datanode和namenode的clusterID不一致。解决方法:先停掉hadoop,把slaves的dfs/data的内容删除,再次启动后,会创建新的clusterID,也可以复制master的clusterID到slaves中。

Q4:出现Exception in createBlockOutputStream XXX:50010 java.io.IOException: Bad connect ack with  firstBadLink as 192.168.1.24:50010

A:查看防火墙状态`systemctl status firewalld.service`,若已经开启先关闭防火墙`systemctl stop firewalld.service`。但往往还是会出现以上错误,再
把master和slaves的tmp,master的dfs/name目录和slaves的dfs/data文件夹内容清空即可。





